# -*- coding: utf-8 -*-
"""IDS_Project_Full.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bF47tLzkG9hSV_N80aD0-bAqgtSCfamg
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sb
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score

data_set = pd.read_csv('Placement_Data_Full_Class.csv')
data_set.head()

data_set.columns

sb.catplot(x='ssc_b', kind='count', data=data_set)
sb.catplot(x='hsc_b', kind='count', data=data_set)
sb.catplot(x='hsc_s', kind='count', data=data_set)
sb.catplot(x='degree_t', kind='count', data=data_set)
data_set['degree_t'].value_counts(normalize=True)
sb.catplot(x='workex', kind='count', data=data_set)
sb.catplot(x='specialisation', kind='count', data=data_set)

data_set.isna().sum()

pl_df = data_set.drop(['sl_no', 'salary'], axis=1)
pl_df.info()

pl_df.describe().transpose()

plt.figure(figsize=(12, 12))
box = sb.boxplot(x='ssc_p', y='status', data=pl_df)

plt.figure(figsize=(12, 12))
box = sb.boxplot(x='degree_p', y='status', data=pl_df)

plt.figure(figsize=(12, 12))
box = sb.boxplot(x='hsc_p', y='status', data=pl_df)

plt.figure(figsize=(12, 12))
box = sb.boxplot(x='etest_p', y='status', data=pl_df)

plt.figure(figsize=(12, 12))
box = sb.boxplot(y='mba_p', x='status', data=pl_df)

# Scatter plot for 'ssc_p' vs 'hsc_p'
plt.figure(figsize=(12, 8))
sb.scatterplot(x='ssc_p', y='hsc_p', hue='status', data=pl_df)
plt.title('Scatter Plot: SSC Percentage vs. HSC Percentage')
plt.xlabel('SSC Percentage')
plt.ylabel('HSC Percentage')
plt.show()

# Scatter plot for 'mba_p' vs 'etest_p'
plt.figure(figsize=(12, 8))
sb.scatterplot(x='mba_p', y='etest_p', hue='status', data=pl_df)
plt.title('Scatter Plot: MBA Percentage vs. Etest Percentage')
plt.xlabel('MBA Percentage')
plt.ylabel('Etest Percentage')
plt.show()

# ... (rest of the code remains the same)

# Encode categorical variables
labels = LabelEncoder()
pl_df['gender'] = labels.fit_transform(pl_df['gender'])
pl_df['ssc_b'] = labels.fit_transform(pl_df['ssc_b'])
pl_df['hsc_b'] = labels.fit_transform(pl_df['hsc_b'])
pl_df['hsc_s'] = labels.fit_transform(pl_df['hsc_s'])
pl_df['degree_t'] = labels.fit_transform(pl_df['degree_t'])
pl_df['workex'] = labels.fit_transform(pl_df['workex'])
pl_df['specialisation'] = labels.fit_transform(pl_df['specialisation'])
pl_df['status'] = labels.fit_transform(pl_df['status'])

plt.figure(figsize=(10, 6))
status_counts = pl_df['status'].value_counts()
labels = ['Placed', 'Not Placed']
colors = ['lightblue', 'lightcoral']
explode = (0.1, 0)  # explode the 1st slice (i.e., 'Placed')

plt.pie(status_counts, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)
plt.title('Placement Status Distribution')
plt.show()



# Plotting a pie chart
plt.figure(figsize=(10, 6))
# Count the number of occurrences for each degree type
degree_counts = pl_df['degree_t'].value_counts()
labels = ['Sci&Tech', 'Commerce', 'Arts']
colors = ['lightblue', 'orange', 'green']

plt.pie(degree_counts, labels=labels, autopct='%1.1f%%', startangle=90, colors=colors)
plt.title('Distribution of Degrees')
plt.show()

# Pair plot
plt.figure(figsize=(15, 15))
sb.pairplot(pl_df, hue='status', palette='viridis')
plt.suptitle('Pair Plot of Features with Status', y=1.02)
plt.show()


# Calculate the correlation matrix
correlation_matrix = pl_df.corr()

# Set up the matplotlib figure
plt.figure(figsize=(12, 10))

# Draw the heatmap using seaborn
sb.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=.5)

# Set the title of the plot
plt.title('Correlation Matrix')

# Show the plot
plt.show()

# Train-test split
X = pl_df.drop(['status'], axis=1)
Y = pl_df['status']
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=.25, random_state=42)

# Logistic Regression
logreg = LogisticRegression(solver='liblinear')
logreg.fit(X_train, y_train)

pred_test = logreg.predict(X_test)

print("Logistic Regression Results:")
print("Test confusion matrix:\n", confusion_matrix(pred_test, y_test))
print("Accuracy:", accuracy_score(y_test, pred_test) * 100)
print("Precision:", precision_score(y_test, pred_test) * 100)
print("Recall:", recall_score(y_test, pred_test) * 100)

# KNN Classifier
classifier = KNeighborsClassifier(n_neighbors=8)
classifier.fit(X_train, y_train)

y_pred = classifier.predict(X_test)

print("\nKNN Classifier Results:")
print("Test confusion matrix:\n", confusion_matrix(y_pred, y_test))
print("Accuracy:", accuracy_score(y_test, y_pred) * 100)
print("Precision:", precision_score(y_test, y_pred) * 100)
print("Recall:", recall_score(y_test, y_pred) * 100)

# SVM Classifier
svc = SVC()
svc.fit(X_train, y_train)

svc_pred = svc.predict(X_test)

print("\nSVM Classifier Results:")
print("Test confusion matrix:\n", confusion_matrix(svc_pred, y_test))
print("Accuracy:", accuracy_score(y_test, svc_pred) * 100)
print("Precision:", precision_score(y_test, svc_pred) * 100)
print("Recall:", recall_score(y_test, svc_pred) * 100)

classifiers = {
    'Logistic Regression': LogisticRegression(solver='liblinear'),
    'K-Nearest Neighbors': KNeighborsClassifier(n_neighbors=8),
    'Support Vector Machine': SVC()
}

results = {'Model': [], 'Accuracy': [], 'Precision': [], 'Recall': []}

for name, classifier in classifiers.items():
    classifier.fit(X_train, y_train)
    y_pred = classifier.predict(X_test)

    accuracy = accuracy_score(y_test, y_pred) * 100
    precision = precision_score(y_test, y_pred) * 100
    recall = recall_score(y_test, y_pred) * 100

    results['Model'].append(name)
    results['Accuracy'].append(accuracy)
    results['Precision'].append(precision)
    results['Recall'].append(recall)

    print(f"{name} - Accuracy: {accuracy:.2f}%, Precision: {precision:.2f}%, Recall: {recall:.2f}%")

# Pie chart for accuracy scores
plt.figure(figsize=(10, 6))
plt.pie(results['Accuracy'], labels=results['Model'], autopct='%1.1f%%', startangle=90)
plt.title('Accuracy Scores of Different Models')
plt.show()